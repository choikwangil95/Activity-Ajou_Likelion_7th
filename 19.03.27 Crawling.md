# 멋쟁이 사자처럼 수업
## Crawling
#### 모듈 : 프로그램의 꾸러미로 package임 / 따라서 만들어진 모듈을 사용하는 것이 편하다<br/>
- 모듈 설치하기 -> pip install 모듈이름<br/>
- 모듈 가져오기 -> from 모듈 import 함수<br/>
- 모듈 사용하기 -> 사용해 그냥<br/>
- 사용한 모듈 : request, bs4, pandas<br/>
#### pandas : 파이썬을 통해 데이터분석을 할 때 분석을 하기 위한 라이브러리 중 하나로 dataframe을 가공하는 것.
#### Request : urlopen 라이브러리를 통해 웹을 연다.
#### BeautifulSoup : html을 python이 이해하는 객체 구조로 만들어주는 parsing을 맡고있는 라이브러리<br/>
####                 이 라이브러리를 이용해 우리는 제대로 된 ‘의미있는’ 정보를 추출해 낼 수 있다.
   (여기서 파싱(Parsing)은 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출하여 
    정보로 가공하는 것을 말하는 것이다 )
#### 모듈 가져오기
```python
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
```
#### 웹에 있는 소스 가져오기
```python
url = "https://music.bugs.co.kr/chart"
html = urlopen(url)
source = html.read() # 바이트코드 type으로 소스를 읽는다.
html.close() # urlopen을 진행한 후에는 close를 한다.
soup = BeautifulSoup(source, "html.parser") 
# 파싱할 문서를 BeautifulSoup 클래스의 생성자에 넘겨주어 문서 개체를 생성, 관습적으로 soup 이라 부름
```
#### 웹에 있는 소스 중 원하는 소스 선택하기
```python
# 웹에 있는 소스 중 원하는 소스 선택하기
table =soup.find("tbody")
songs = table.find_all(class_="left")
songlist=['벅스 음원 차트']
# name = soup.select(h3>a) 특정 html 문장을 선택하는 것
```
#### 정보 가공하기
```python
rank=1
for song in songs:
   
    title = song.get_text()
    title = title.replace("\n", "")  
    songlist.append('위 : {}'.format(title))
    print('{}위 : {}'.format(rank,title))
    rank +=1
    if rank == 101:
        break
```
#### dataframe 가공하기
```python
import pandas as pd

data = pd.DataFrame(songlist)
header=['순위, 노래제목']
data.to_csv('bugs_chart.csv',encoding='utf-16',header=header)
```
#### Beautifulsoup 함수 알아보장
```python
find() 및 find_all()함수 -> 함수 인자로는 태그의 이름, 속성 기타등등이 들어간다.
ex) test=soup.find("p") test2=test.find_all(class_="title")
-> p 태그를 찾고 p 태그 중 class가 title인 것들을 선택해라.

ex) 태그와 속성을 같이 가져오려면 test=soup.find('p',{'class':'title'}) 이렇게 해준다.
```
## Login
### 웹 사이트를 로그인 하는데 있어 쿠키와 세션을 빼놓고 이야기하는 것은 불가능하다. 이게 뭐야..
[쿠키와 세션](https://beomi.github.io/beomi.github.io_old/python/2017/01/20/HowToMakeWebCrawler-With-Login.html taget="_blank")
#### Selenium은 주로 웹앱을 테스트하는데 이용하는 프레임워크다.<br/>
#### Selenium은 웹 테스트 자동화 도구이지만, 멋진 크롤링 도구로 사용할 수 있다.<br/>
#### webdriver라는 API를 통해 운영체제에 설치된 Chrome등의 브라우저를 제어하게 된다.(Chromedriver를 설치해야한다)
#### 모듈 가져오기
```python
from selenium import webdriver
from bs4 import BeautifulSoup
```
#### driver라는 이름의 webdriver 객체를 만들어주자
```python
# Chrome의 경우 | 아까 받은 chromedriver의 위치를 지정해준다.
driver = webdriver.Chrome('C:\\Users\\광일\\Desktop\\chromedriver_win32\\chromedriver.exe')
driver.implicitly_wait(3) #모든 웹자원이 로드될때 까지 기다리게 하는 시간을 지정하는 것(3초)
```
#### 특정 url로 브라우저를 키고 
```python
driver.get('https://class.likelion.org/accounts/login/?next=/board/notices/')
## 아이디/비밀번호를 입력해준다.
driver.find_element_by_name('username').send_keys('choikwangil0@likelion.org')
driver.find_element_by_name('password').send_keys('26619693')
driver.find_element_by_xpath("//button[@type='submit']").click() # 버튼클릭하기
```
#### Selenium method
- get() : url에 접근하는 api
- find_element_by_name(): 페이지의 단일 element에 접근하는 api
- find_element_by_name(): 페이지의 여러 elements에 접근하는 api
위의 메소드들은 html을 브라우저에서 파싱해주기 때문에 굳이 soup안해줘도 된다.

## 로그인 후 로그인이 필요한 페이지 가져오기
```python
driver.get('https://order.pay.naver.com/home') # 로그인이 필요한 페이지 들어가기
source = driver.page_source # 페이지의 elements모두 가져오기
soup = BeatifulSoup(html, 'html.parser') # BeautifulSoup사용하기
notices = soup.select('div.p_inr > div.p_info > a > span')

for n in notices:
    print(n.text.strip())
```
